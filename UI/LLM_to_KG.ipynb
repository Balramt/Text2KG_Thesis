{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a75ae446-576e-4a9e-88fc-f7bf5c596614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 10:22:06.106485: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-25 10:22:06.125154: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-25 10:22:06.131127: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-25 10:22:06.145252: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-25 10:22:07.132877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14516817-4d5f-40a0-9c56-84a313bfab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06de3d8f-ce22-4f5b-bd9a-a3482415df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46dbbe8163044779188305918b09325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization\n",
    "    device_map=\"auto\",  # Automatically place model layers on GPU\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1539c302-5f34-4f53-98c5-cb7505b4bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text-generation pipeline\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758cc5f7-4e68-4b27-9add-d5c342341383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(prompt_text, model_name, max_length=1000, num_return_sequences=1):\n",
    "    if model_name == \"LLama 3\":\n",
    "        response = text_generator(prompt_text, max_length=max_length, num_return_sequences=num_return_sequences)\n",
    "    else:\n",
    "        print(\"No model execution provided\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c921f99a-b9b4-4b53-aebe-a258da20858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_output(response):\n",
    "    if response and len(response) > 0:\n",
    "        generated_text = response[0].get('generated_text', '')\n",
    "        # Extract the text after the last occurrence of \"Test Output:\"\n",
    "        match = re.search(r'Test Output:\\s*(.*)', generated_text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip().split('\\n')\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df000d9f-281c-4ee9-acda-4b9075f8c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse model output\n",
    "def parse_model_output(model_output):\n",
    "    triples = []\n",
    "    for output in model_output:\n",
    "        lines = [line.strip() for line in output.strip().split('\\n')]\n",
    "        pattern = re.compile(r'(.+?)\\s*\\(([^,]+),\\s*([^)]+)\\)')\n",
    "        for line in lines:\n",
    "            matches = pattern.findall(line)\n",
    "            for match in matches:\n",
    "                relation, subject, obj = match\n",
    "                subject = subject.strip()\n",
    "                obj = obj.strip()\n",
    "                triples.append({\"sub\": subject, \"rel\": relation, \"obj\": obj})\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6580e21-ecb5-4033-acd3-1452c659f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process file and generate RDF triples\n",
    "def process_file(file_path, model_name):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Here is the RDF data:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    model_response = generate_text(prompt, model_name, max_length=1000)\n",
    "    test_output = extract_test_output(model_response)\n",
    "    triples = parse_model_output(test_output)\n",
    "    \n",
    "    # Format triples for output\n",
    "    formatted_triples = \"\\n\".join([f\"{triple['rel']}({triple['sub']}, {triple['obj']})\" for triple in triples])\n",
    "    \n",
    "    # Call function to generate and save knowledge graph\n",
    "    image_path = generate_knowledge_graph(triples)\n",
    "\n",
    "    return formatted_triples, image_path  # Return formatted triples and image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a7d21a-cf08-4087-b189-01bf83c3afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate knowledge graph\n",
    "def generate_knowledge_graph(rdf_triples):\n",
    "    G = nx.DiGraph()\n",
    "    for triple in rdf_triples:\n",
    "        G.add_edge(triple['sub'], triple['obj'], relation=triple['rel'])\n",
    "\n",
    "    pos = nx.spring_layout(G, seed=42, k=1.5)\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=2500, node_color='lightblue', edgecolors='black')\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=G.edges(), arrowstyle='->', arrowsize=30, edge_color='black')\n",
    "    nx.draw_networkx_labels(G, pos, font_size=7, font_family='sans-serif', font_weight='bold')\n",
    "    edge_labels = {(u, v): d['relation'] for u, v, d in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red', label_pos=0.5, font_size=8, rotate=True)\n",
    "    plt.title('Knowledge Graph', fontsize=20, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Save the figure to a file\n",
    "    image_path = \"KG_images/knowledge_graph.png\"  # Adjust the path if needed\n",
    "    plt.savefig(image_path)\n",
    "    plt.close()  # Close the plot to free memory\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebc8f2ae-6012-40ea-9377-fc0936fe79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# RDF Triple Generator\")\n",
    "    gr.Markdown(\"Upload a text file, select a model, and generate RDF triples using LLama models.\")\n",
    "\n",
    "    # File upload component with filepath type\n",
    "    file_input = gr.File(label=\"Upload Input Text File\", type=\"filepath\")\n",
    "    \n",
    "    # Dropdown menu for model selection\n",
    "    selected_model = gr.Dropdown(choices=[\"LLama 3\", \"Mistral\"], label=\"Select Model\")\n",
    "\n",
    "    # Generate button to trigger RDF generation\n",
    "    generate_button = gr.Button(\"Generate RDF Triples\")\n",
    "    \n",
    "    # Textbox to display the extracted triples\n",
    "    triples_output = gr.Textbox(label=\"Extracted RDF Triples\", lines=10)\n",
    "    \n",
    "    # Image output component to display the generated knowledge graph\n",
    "    graph_output = gr.Image(label=\"Knowledge Graph\", type=\"filepath\")\n",
    "\n",
    "    # Connect the button to the processing function\n",
    "    generate_button.click(fn=process_file, inputs=[file_input, selected_model], outputs=[triples_output, graph_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f0a96c0-67f0-45e7-9504-52e6c4bcbcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://8460aa9516e170c36a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8460aa9516e170c36a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/upb/users/b/balram/profiles/unix/cs/.local/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Launch the Gradio interface\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21058456-efe1-4f44-89de-c14a1317163d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
