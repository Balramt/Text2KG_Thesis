{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "868c5ade-2f4f-45cb-a039-30ee66f6b02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True if CUDA is available\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d118aa3b-f294-408e-a7db-d412d99d515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aa681bb-4193-429d-8344-547d0acaab14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep 25 18:58:58 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti     On  |   00000000:81:00.0 Off |                  N/A |\n",
      "| 41%   73C    P2             85W /  250W |   10043MiB /  11264MiB |     95%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   2489360      C   ...niforge3/envs/jupyterhub/bin/python      10040MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b76c240c-88af-4b37-9d85-b67d74a1719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 2450845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f7694c7-36b7-4084-9a89-2b3837bdf56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 18:13:57.649593: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 18:13:57.668890: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 18:13:57.674953: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 18:13:57.690609: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 18:13:58.721434: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, pipeline, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e82cf9-963c-4c73-9ff1-129a57ebb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e20da5-a7af-44fa-ac94-4806bc0a4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c2bfeed-2202-4dff-b72f-2e9e8641ca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/upb/users/b/balram/profiles/unix/cs/.local/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f167f5be-e474-4675-bfb3-48100098e59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1bc73060064a6086ec8ee80a019895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84abf0d8-c4e6-4480-ac7d-e934aa175b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation\n",
    "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e13eaac-7796-48f5-83e9-c748dd48fcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate text based on a prompt\n",
    "prompt = \"Who is Ada Lovelace?\"\n",
    "generated_text = text_generator(prompt, max_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ef13a18-28bc-47b4-8672-0d15b95f65c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Ada Lovelace?\n",
      "\n",
      "Ada Lovelace was an English mathematician and writer, chiefly known for her work on Charles Babbage's early mechanical general-purpose computer, the Analytical Engine. Her notes on the engine include what is recognized as the first algorithm intended to be processed by a machine, and she is often regarded as the first computer programmer.\n",
      "\n",
      "Lovelace was the daughter of the poet Lord Byron and his wife Anne Isabella Milbanke. She was named after her mother, Ada Byron, who was given the name Ada in honor of Countess Ada Lovelace, a friend of her grandmother. Lovelace was educated at home by various tutors, including Mary Somerville, and showed a talent for mathematics from an early age.\n",
      "\n",
      "In 1833, Lovelace met Charles Babbage, who was working on the design of the Analytical Engine. Babbage was impressed by Lovelace's mathematical abilities and asked her to translate an article on the engine by Italian mathematician Luigi Menabrea. In the process of translating the article, Lovelace added extensive notes of her own, which included a detailed description of how the engine could be used to compute Bernoulli numbers, and a program for calculating the sequence of numbers generated by the Bernoulli function. This program is considered to be the first algorithm intended to be processed by a machine, and Lovelace is often referred to as the first computer programmer.\n",
      "\n",
      "Lovelace continued to work on the Analytical Engine for several years, and her notes on the machine were published in 1843. She also wrote a number of other mathematical papers, and was a member of several scientific societies.\n",
      "\n",
      "Lovelace died in 1852 at the age of 36, after a long illness. She is remembered as a pioneer of computer science, and her work on the Analytical Engine is still studied and admired today. In 1980, she was posthumously named as the first computer programmer by the Association for Computing Machinery, and in 2013, she was honored with a Google Doodle on what would have been her 200th birthday.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the result\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1104a75-057d-4a3a-86ca-b608a0a69e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b18074-bf71-473c-9d1b-f6c3d85a3789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abef6db0-557f-4aef-8ea3-a6ea0f93c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "173ebd61-eff7-45a3-90f8-4d94c8e31cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdbd5854-6b23-472a-b123-34b54c3063d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8f60989-fa09-4a56-b15a-5db0f4095846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts_data(filepath):\n",
    "    with jsonlines.open(filepath) as reader:\n",
    "        return list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df61a9df-769d-4a3f-85c8-f1fadcfef8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompt_info(item):\n",
    "    prompt_id = item.get('id')\n",
    "    prompt_text = item.get('prompt')\n",
    "   #print(\"prompt_text\",prompt_text)\n",
    "    return prompt_id, prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "099393fa-e2d7-4931-8fa8-36fa3981f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'text_generator' is already defined somewhere in your environment\n",
    "def generate_text(prompt_text, max_length=500, num_return_sequences=1):\n",
    "    response = text_generator(prompt_text, max_length=max_length, num_return_sequences=num_return_sequences)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8ba4201-9a02-4784-b7e5-7d5ec0f40c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_output(response):\n",
    "    #print(\"response\",response)\n",
    "    if response and len(response) > 0:\n",
    "        generated_text = response[0].get('generated_text', '')\n",
    "        match = re.search(r'Test Output:\\s*(.*?)\\s*(?:\\n|$)', generated_text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    return 'Output not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76508caa-b549-41b2-a85b-4957feba34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_output(model_output):\n",
    "    # Initialize an empty list to store triples\n",
    "    triples = []\n",
    "    \n",
    "    # Remove leading/trailing whitespace and split the input data into lines\n",
    "    lines = [line.strip() for line in model_output.strip().split('\\n')]\n",
    "    \n",
    "    # Define the regex pattern to match triples in the format: relation(subject, object)\n",
    "    pattern = re.compile(r'(.+?)\\s*\\(([^,]+),\\s*([^)]+)\\)')\n",
    "    \n",
    "    for line in lines:\n",
    "        # Find all matches for the pattern in the line\n",
    "        matches = pattern.findall(line)\n",
    "        \n",
    "        for match in matches:\n",
    "            relation, subject, obj = match\n",
    "            # Clean up subject and object values\n",
    "            subject = subject.strip()\n",
    "            obj = obj.strip()\n",
    "            \n",
    "            # Append the extracted triple to the list\n",
    "            triples.append({\"sub\": subject, \"rel\": relation, \"obj\": obj})\n",
    "    \n",
    "    # Return the list of triples\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e442fad-f5fc-4eca-a2e6-20f5cf545d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONL_FILEPATH = 'Wikidata/Input_Prompts/ont_10_culture_prompts.jsonl'\n",
    "output_filepath='Wikidata/Response/ont_10_culture_llm_response.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34e396b9-d98d-4f7f-bf03-e01a7823ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_triples_to_jsonl(processed_data, output_filepath):\n",
    "    \"\"\"\n",
    "    Save the processed triples into a JSONL file format.\n",
    "\n",
    "    Args:\n",
    "        processed_data (list): A list of dictionaries containing the 'id' and 'triples'.\n",
    "        output_filepath (str): The output file path where the JSONL data should be saved.\n",
    "    \"\"\"\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "        for entry in processed_data:\n",
    "            jsonl_entry = {\n",
    "                \"id\": entry[\"id\"],\n",
    "                \"triples\": entry[\"triples\"]\n",
    "            }\n",
    "            # Convert dictionary to a JSON string and write it to the file\n",
    "            outfile.write(json.dumps(jsonl_entry, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3aaafe7c-80af-46b9-a575-3d89439d15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(filepath, output_filepath, num_prompts=4):\n",
    "    prompts_data = load_prompts_data(filepath)\n",
    "    processed_data = []\n",
    "    for i in range(min(num_prompts, len(prompts_data))):\n",
    "        item = prompts_data[i]\n",
    "        prompt_id, prompt_text = extract_prompt_info(item)\n",
    "        response = generate_text(prompt_text)\n",
    "        test_output = extract_test_output(response)\n",
    "        \n",
    "        # Debugging: Print the test output to ensure it's correctly extracted\n",
    "        print(f\"Test Output for ID {prompt_id}: {test_output}\")\n",
    "        \n",
    "        # Parse the test output into triples\n",
    "        triples = parse_model_output(test_output)\n",
    "        \n",
    "        # Debugging: Print the parsed triples to ensure they're correct\n",
    "        print(f\"Parsed Triples for ID {prompt_id}: {triples}\")\n",
    "        \n",
    "        processed_entry = {\n",
    "            \"id\": prompt_id,\n",
    "            \"triples\": triples\n",
    "        }\n",
    "        \n",
    "        processed_data.append(processed_entry)\n",
    "    \n",
    "    # Save the processed data into a new jsonl file\n",
    "    save_triples_to_jsonl(processed_data, output_filepath)\n",
    "    print(f\"Processed triples saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38bcb6-a4be-4871-9f54-6519f7ca3324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Replace 'your_input_filepath.jsonl' with your actual input file path\n",
    "# Replace 'your_output_filepath.jsonl' with your desired output file path\n",
    "main(JSONL_FILEPATH, output_filepath, num_prompts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561a64e-ded7-44ca-a1ba-f83af42f3706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f3fcf9-d26d-4abc-9b9d-aa8f38d820c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
