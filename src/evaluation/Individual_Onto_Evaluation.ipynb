{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "499a7519-9c5b-4952-9d88-a9e059cecb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f35d688-870b-4d9b-b69c-c9bad6315dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "354a6050-5ef0-4540-9171-73528414e347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /upb/users/b/balram/profiles/unix/cs/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ba77daf-df2c-47e8-8f33-a93fe2fe51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_f1(gold, pred):\n",
    "    if len(pred) == 0:\n",
    "        return 0, 0, 0\n",
    "    p = len(gold.intersection(pred)) / len(pred)\n",
    "    r = len(gold.intersection(pred)) / len(gold)\n",
    "    f1 = 2 * ((p * r) / (p + r)) if (p + r) > 0 else 0\n",
    "    return p, r, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf9eca5f-b794-40be-b7fa-4de8e5bdb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_object_hallucinations(ps, test_sentence, triples):\n",
    "    if len(triples) == 0:\n",
    "        return 0, 0\n",
    "\n",
    "    stemmed_sentence = \"\".join([ps.stem(word) for word in word_tokenize(test_sentence)])\n",
    "    normalized_stemmed_sentence = re.sub(r\"(_|\\s+)\", '', stemmed_sentence).lower()\n",
    "\n",
    "    num_subj_hallucinations, num_obj_hallucinations = 0, 0\n",
    "    for triple in triples:\n",
    "        normalized_stemmed_subject = clean_entity_string(ps, triple[0])\n",
    "        normalized_stemmed_object = clean_entity_string(ps, triple[2])\n",
    "\n",
    "        if normalized_stemmed_sentence.find(normalized_stemmed_subject) == -1:\n",
    "            num_subj_hallucinations += 1\n",
    "        if normalized_stemmed_sentence.find(normalized_stemmed_object) == -1:\n",
    "            num_obj_hallucinations += 1\n",
    "\n",
    "    subj_hallucination = num_subj_hallucinations / len(triples)\n",
    "    obj_hallucination = num_obj_hallucinations / len(triples)\n",
    "    return subj_hallucination, obj_hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f16519e-6b20-4027-965f-5c7344b525ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ontology_conformance(ontology_rels, triples):\n",
    "    if len(triples) == 0:\n",
    "        return 1, 0\n",
    "    num_rels_conformant = len([tr for tr in triples if tr[1] in ontology_rels])\n",
    "    ont_conformance = num_rels_conformant / len(triples)\n",
    "    rel_hallucination = 1 - ont_conformance\n",
    "    return ont_conformance, rel_hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6392bf30-4cf3-49fc-8eff-7e1c07c151ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_triple(sub_label, rel_label, obj_label):\n",
    "    sub_label = re.sub(r\"(_|\\s+)\", '', sub_label).lower()\n",
    "    rel_label = re.sub(r\"(_|\\s+)\", '', rel_label).lower()\n",
    "    obj_label = re.sub(r\"(_|\\s+)\", '', obj_label).lower()\n",
    "    return f\"{sub_label}{rel_label}{obj_label}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39813981-7909-47e2-8597-6b5f26e56b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entity_string(ps, entity):\n",
    "    stemmed_entity = \"\".join([ps.stem(word) for word in word_tokenize(entity)])\n",
    "    normalized_stemmed_entity = re.sub(r\"(_|\\s+)\", '', stemmed_entity).lower()\n",
    "    return normalized_stemmed_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd838b69-30a9-449c-8d51-96413996aba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43226bd9-4b05-447e-be7a-0ab0a4b75d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save_results(ground_truth_data, model_data, output_file):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    results = []\n",
    "    for gt_entry, model_entry in zip(ground_truth_data, model_data):\n",
    "        gt_triples = [[tr['sub'], tr['rel'], tr['obj']] for tr in gt_entry['triples']]\n",
    "        system_triples = [[tr['sub'], tr['rel'], tr['obj']] for tr in model_entry['triples']]\n",
    "       # print('gt_triples',gt_triples)\n",
    "       # print('system_triples',system_triples)\n",
    "\n",
    "        gt_relations = {tr[1].replace(\" \", \"_\") for tr in gt_triples}\n",
    "        filtered_system_triples = [tr for tr in system_triples if tr[1] in gt_relations]\n",
    "\n",
    "        #print('gt_relations',gt_relations)\n",
    "        #print('filtered_system_triples',filtered_system_triples)\n",
    "        \n",
    "        normalized_gt_triples = {normalize_triple(tr[0], tr[1], tr[2]) for tr in gt_triples}\n",
    "        #print('normalized_gt_triples',normalized_gt_triples)\n",
    "        normalized_system_triples = {normalize_triple(tr[0], tr[1], tr[2]) for tr in filtered_system_triples}\n",
    "        #print('normalized_system_triples',normalized_system_triples)\n",
    "\n",
    "        precision, recall, f1 = calculate_precision_recall_f1(normalized_gt_triples, normalized_system_triples)\n",
    "        ont_conformance, rel_hallucination = get_ontology_conformance(gt_relations, system_triples)\n",
    "        subj_hallucination, obj_hallucination = get_subject_object_hallucinations(ps, gt_entry['sent'], system_triples)\n",
    "\n",
    "        result = {\n",
    "            \"id\": gt_entry['id'],\n",
    "            \"precision\": f\"{precision:.2f}\",\n",
    "            \"recall\": f\"{recall:.2f}\",\n",
    "            \"f1\": f\"{f1:.2f}\",\n",
    "            \"onto_conf\": f\"{ont_conformance:.2f}\",\n",
    "            \"rel_halluc\": f\"{rel_hallucination:.2f}\",\n",
    "            \"sub_halluc\": f\"{subj_hallucination:.2f}\",\n",
    "            \"obj_halluc\": f\"{obj_hallucination:.2f}\",\n",
    "            \"llm_triples\": system_triples,\n",
    "            \"filtered_llm_triples\": filtered_system_triples,\n",
    "            \"gt_triples\": gt_triples,\n",
    "            \"sent\": gt_entry['sent']\n",
    "        }\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for result in results:\n",
    "            f.write(json.dumps(result) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0f8f79e-589c-4c0b-9eba-8e8ca28c060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ground_truth_jsonl(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "        # Validate each entry to ensure it has 'id', 'sent', and 'triples'\n",
    "        for entry in data:\n",
    "            if not all(key in entry for key in ['id', 'sent', 'triples']):\n",
    "                raise ValueError(f\"Entry missing required keys: {entry}\")\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65fcfe11-b72e-437a-90d9-f4a6c9f7324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_jsonl(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "        # Validate each entry to ensure it has 'id' and 'triples'\n",
    "        for entry in data:\n",
    "            if not all(key in entry for key in ['id', 'triples']):\n",
    "                raise ValueError(f\"Entry missing required keys: {entry}\")\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33a2eef7-922c-48c9-bb4f-f7f2cabcdc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filepath='Wikidata/Evaluation_Statistics/ont_9_nature_llm_stats.jsonl'\n",
    "ground_truth_filepath='Wikidata/Ground_Truth/ont_9_nature_ground_truth.jsonl'\n",
    "model_response_filepath='Wikidata/Response/ont_9_nature_llm_response.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da01cd0e-60f9-4ee9-834f-e68b83dda384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "ground_truth_data = read_ground_truth_jsonl(ground_truth_filepath)\n",
    "model_data = read_model_jsonl(model_response_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9e729ff-2380-46f6-87d7-85906f0e85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_save_results(ground_truth_data, model_data, output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691b54e-8227-4a2e-a83a-5f93e8d7de7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
